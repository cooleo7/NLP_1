{"cells":[{"cell_type":"code","execution_count":null,"id":"62cc0b42","metadata":{"id":"62cc0b42"},"outputs":[],"source":["import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","import random\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from string import punctuation\n","from sklearn.preprocessing import OneHotEncoder\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import time\n","\n","%config IPCompleter.greedy=True\n","%config IPCompleter.use_jedi=False"]},{"cell_type":"markdown","id":"214e9196","metadata":{"id":"214e9196"},"source":["# (case1) dataset : output_reviews_PA_FL (same as 1st model) & 3 convolution layers with tanh & fully cnnected layer with relu"]},{"cell_type":"code","execution_count":null,"id":"afc62d64","metadata":{"id":"afc62d64","outputId":"6d2afb38-9e47-4899-fca6-94727fbb89ac"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/1859262419.py:16: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["mode:  binary\n","0 epoch : accuracy 0.100000\n","100 epoch : accuracy 0.860000\n","200 epoch : accuracy 0.980000\n","accuracy : 0.375000\n","mode:  count\n","0 epoch : accuracy 0.020000\n","100 epoch : accuracy 0.880000\n","200 epoch : accuracy 1.000000\n","accuracy : 0.300000\n","mode:  tfidf\n","0 epoch : accuracy 0.020000\n","100 epoch : accuracy 1.000000\n","200 epoch : accuracy 1.000000\n","accuracy : 0.335000\n","mode:  freq\n","0 epoch : accuracy 0.020000\n","100 epoch : accuracy 0.500000\n","200 epoch : accuracy 0.740000\n","accuracy : 0.260000\n","\n","                                     binary  \\\n","0  tf.Tensor(37.5, shape=(), dtype=float32)   \n","\n","                                           count  \\\n","0  tf.Tensor(30.000002, shape=(), dtype=float32)   \n","\n","                                      tfidf  \\\n","0  tf.Tensor(33.5, shape=(), dtype=float32)   \n","\n","                                       freq  \n","0  tf.Tensor(26.0, shape=(), dtype=float32)  \n"]}],"source":["review = pd.read_csv('output_reviews_PA_FL.csv')\n","\n","del review['review_id']\n","del review['user_id']\n","del review['business_id']\n","del review['useful']\n","del review['funny']\n","del review['cool']\n","del review['date']\n","del review['state']\n","del review['Unnamed: 0.1']\n","del review['Unnamed: 0']\n","\n","review = review[:1000]\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.optimizers.legacy import SGD\n","\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","#########################\n","# Define the vocabulary #\n","#########################\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","\n","def add_doc_to_vocab(docs, vocab):\n","    # input: docs: a list of sentences / vocab: a vocabulary dictionary\n","\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab # updated vocabulary\n","\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","\n","#########################\n","# embedding             #\n","#########################\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","\n","    if mode == 'count':\n","        scaler=MinMaxScaler()\n","        scaler.fit(Xtrain)\n","        Xtrain=scaler.transform(Xtrain)\n","        scaler.fit(Xtest)\n","        Xtest=scaler.transform(Xtest)\n","\n","    return Xtrain, Xtest\n","\n","\n","#########################\n","# CNN                   #\n","#########################\n","class CNN(tf.keras.Model):\n","    def __init__(self, f):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='tanh', input_shape=(f,1))\n","        self.pool1 = tf.keras.layers.MaxPool1D(2)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='tanh')\n","        self.pool2 = tf.keras.layers.MaxPool1D(2)\n","        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='tanh')\n","        self.pool3 = tf.keras.layers.MaxPool1D(2)\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.fullyconn = tf.keras.layers.Dense(1024, activation='relu')\n","\n","        self.out = tf.keras.layers.Dense(units=10, activation=None)\n","\n","    def call(self, x):\n","        o_conv1 = self.conv1(x)\n","        o_pool1 = self.pool1(o_conv1)\n","        o_conv2 = self.conv2(o_pool1)\n","        o_pool2 = self.pool2(o_conv2)\n","        o_conv3 = self.conv3(o_pool2)\n","        o_pool3 = self.pool3(o_conv3)\n","        o_flat = self.flatten(o_pool3)\n","        o_dropout = self.dropout(o_flat)\n","        o_fc = self.fullyconn(o_dropout)\n","        logits = self.out(o_fc)\n","        y_pred = tf.nn.softmax(logits)\n","\n","        return y_pred, logits\n","\n","\n","# accuracy\n","def compute_accuracy(y_pred, y):\n","    corr_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n","    acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n","    return acc\n","\n","\n","# loss function\n","def cross_entropy_loss(logits, y):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","\n","\n","# optimisation\n","optimiser = tf.keras.optimizers.legacy.Adam(1e-4)\n","def tune_param(model, x, y):\n","    with tf.GradientTape() as tape:\n","        y_pred, logits = model(x)\n","        loss = cross_entropy_loss(logits, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","\n","\n","# apply one-hot encoding to a target value\n","train_y, test_y = tf.one_hot(train_y, depth=10), tf.one_hot(test_y, depth=10)\n","\n","\n","# Run Experiment of 4 different modes\n","modes =  ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using different mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train data reconstruction (shuffle -> batch form)\n","    train_data = tf.data.Dataset.from_tensor_slices((Xtrain, train_y))\n","    train_data = train_data.repeat().shuffle(100).batch(50)\n","    train_data_iter = iter(train_data)\n","\n","    CNN_model = CNN(Xtrain.shape[1])\n","    # optimise 200 epoch , but try optimise 10000 epoch in HPC\n","    for i in range(201):\n","        # assign each 50 data\n","        batch_x, batch_y = next(train_data_iter)\n","\n","        if i % 100 == 0:\n","            train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n","            print(\"%d epoch : accuracy %f\" % (i, train_accuracy))\n","\n","        # parameter update\n","        tune_param(CNN_model, batch_x, batch_y)\n","\n","\n","    test_acc = compute_accuracy(CNN_model(Xtest)[0], test_y)\n","    print(\"accuracy : %f\" %test_acc)\n","    results[mode] = [test_acc*100]\n","\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"08835fad","metadata":{"id":"08835fad"},"source":["# (case2) dataset : output_reviews_PA_FL (same as 1st model) & 3 convolution layers with relu & fully cnnected layer with tanh"]},{"cell_type":"code","execution_count":null,"id":"a9a33697","metadata":{"id":"a9a33697","outputId":"3345f04b-b132-48f7-9ed8-884868a3b77b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/4256912923.py:24: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["mode:  binary\n","0 epoch : accuracy 0.260000\n","100 epoch : accuracy 0.300000\n","200 epoch : accuracy 0.440000\n","accuracy : 0.275000\n","mode:  count\n","0 epoch : accuracy 0.100000\n","100 epoch : accuracy 0.380000\n","200 epoch : accuracy 0.320000\n","accuracy : 0.230000\n","mode:  tfidf\n","0 epoch : accuracy 0.240000\n","100 epoch : accuracy 0.320000\n","200 epoch : accuracy 0.480000\n","accuracy : 0.285000\n","mode:  freq\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.380000\n","200 epoch : accuracy 0.240000\n","accuracy : 0.225000\n","\n","                                     binary  \\\n","0  tf.Tensor(27.5, shape=(), dtype=float32)   \n","\n","                                      count  \\\n","0  tf.Tensor(23.0, shape=(), dtype=float32)   \n","\n","                                      tfidf  \\\n","0  tf.Tensor(28.5, shape=(), dtype=float32)   \n","\n","                                       freq  \n","0  tf.Tensor(22.5, shape=(), dtype=float32)  \n"]}],"source":["from collections import Counter\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","review = pd.read_csv('output_reviews_PA_FL.csv')\n","\n","del review['review_id']\n","del review['user_id']\n","del review['business_id']\n","del review['useful']\n","del review['funny']\n","del review['cool']\n","del review['date']\n","del review['state']\n","del review['Unnamed: 0.1']\n","del review['Unnamed: 0']\n","\n","review = review[:1000]\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","\n","\n","#########################\n","# Define the vocabulary #\n","#########################\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","def add_doc_to_vocab(docs, vocab):\n","\n","    # input: docs: a list of sentences / vocab: a vocabulary dictionary\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab  # updated vocabulary\n","\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","\n","#########################\n","# embedding             #\n","#########################\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","\n","    if mode == 'count':\n","        scaler=MinMaxScaler()\n","        scaler.fit(Xtrain)\n","        Xtrain=scaler.transform(Xtrain)\n","        scaler.fit(Xtest)\n","        Xtest=scaler.transform(Xtest)\n","\n","    return Xtrain, Xtest\n","\n","\n","#########################\n","# CNN                   #\n","#########################\n","class CNN(tf.keras.Model):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu')\n","        self.pool1 = tf.keras.layers.MaxPool1D(2)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu')\n","        self.pool2 = tf.keras.layers.MaxPool1D(2)\n","        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu')\n","        self.pool3 = tf.keras.layers.MaxPool1D(2)\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.fullyconn = tf.keras.layers.Dense(1024, activation='tanh')\n","\n","        self.out = tf.keras.layers.Dense(units=5, activation=None)\n","\n","    def call(self, x):\n","        o_conv1 = self.conv1(x)\n","        o_pool1 = self.pool1(o_conv1)\n","        o_conv2 = self.conv2(o_pool1)\n","        o_pool2 = self.pool2(o_conv2)\n","        o_conv3 = self.conv3(o_pool2)\n","        o_pool3 = self.pool3(o_conv3)\n","        o_flat = self.flatten(o_pool3)\n","        o_dropout = self.dropout(o_flat)\n","        o_fc = self.fullyconn(o_dropout)\n","        logits = self.out(o_fc)\n","        y_pred = tf.nn.softmax(logits)\n","\n","        return y_pred, logits\n","\n","\n","# accuracy\n","def compute_accuracy(y_pred, y):\n","    corr_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n","    acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n","\n","    return acc\n","\n","\n","# loss function\n","def cross_entropy_loss(logits, y):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","\n","\n","#optimisation\n","optimiser = tf.keras.optimizers.legacy.Adam(1e-4)\n","def tune_param(model, x, y):\n","    with tf.GradientTape() as tape:\n","        y_pred, logits = model(x)\n","        loss = cross_entropy_loss(logits, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","\n","\n","# apply one-hot encoding to a target value\n","train_y, test_y = tf.one_hot(train_y, depth=5), tf.one_hot(test_y, depth=5)\n","\n","\n","# Run Experiment of 4 different modes\n","modes =  ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using freq mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train data reconstruction (shuffle -> batch form)\n","    train_data = tf.data.Dataset.from_tensor_slices((Xtrain, train_y))\n","    train_data = train_data.repeat().shuffle(60000).batch(50)\n","    train_data_iter = iter(train_data)\n","\n","    CNN_model = CNN()\n","    # optimise 200 epoch , but try optimise 10000 epoch in HPC\n","    for i in range(201):\n","        # assign each 50 data\n","        batch_x, batch_y = next(train_data_iter)\n","\n","        if i % 100 == 0:\n","            train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n","            print(\"%d epoch : accuracy %f\" % (i, train_accuracy))\n","\n","        # parameter update\n","        tune_param(CNN_model, batch_x, batch_y)\n","\n","\n","    test_acc = compute_accuracy(CNN_model(Xtest)[0], test_y)\n","    print(\"accuracy : %f\" %test_acc)\n","    results[mode] = [test_acc*100]\n","\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"0176b3ac","metadata":{"id":"0176b3ac"},"source":["# (case3) dataset : review_test.csv (sample from original yelp data) & 3 convolution layers with tanh & fully cnnected layer with relu"]},{"cell_type":"code","execution_count":null,"id":"40366845","metadata":{"id":"40366845","outputId":"60f7ccf8-f8da-4085-fc6d-b065ec816556"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/1375314544.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["mode:  binary\n","0 epoch : accuracy 0.100000\n","100 epoch : accuracy 0.420000\n","200 epoch : accuracy 0.680000\n","accuracy : 0.572864\n","mode:  count\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.720000\n","200 epoch : accuracy 0.940000\n","accuracy : 0.497487\n","mode:  tfidf\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.900000\n","200 epoch : accuracy 0.980000\n","accuracy : 0.562814\n","mode:  freq\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.480000\n","200 epoch : accuracy 0.620000\n","accuracy : 0.517588\n","\n","                                         binary  \\\n","0  tf.Tensor(57.28643, shape=(), dtype=float32)   \n","\n","                                          count  \\\n","0  tf.Tensor(49.74874, shape=(), dtype=float32)   \n","\n","                                           tfidf  \\\n","0  tf.Tensor(56.281406, shape=(), dtype=float32)   \n","\n","                                            freq  \n","0  tf.Tensor(51.758797, shape=(), dtype=float32)  \n"]}],"source":["\n","review = pd.read_csv('review_test.csv')\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","del review['Unnamed: 0']\n","\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","\n","#########################\n","# Define the vocabulary #\n","#########################\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","\n","def add_doc_to_vocab(docs, vocab):\n","    # docs: a list of sentences (docs)\n","    # vocab: a vocabulary dictionary\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab # updated vocabulary\n","\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","\n","#########################\n","# embedding             #\n","#########################\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","\n","    if mode == 'count':\n","        scaler=MinMaxScaler()\n","        scaler.fit(Xtrain)\n","        Xtrain=scaler.transform(Xtrain)\n","        scaler.fit(Xtest)\n","        Xtest=scaler.transform(Xtest)\n","\n","    return Xtrain, Xtest\n","\n","\n","#########################\n","# CNN                   #\n","#########################\n","class CNN(tf.keras.Model):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='tanh')\n","        self.pool1 = tf.keras.layers.MaxPool1D(2)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='tanh')\n","        self.pool2 = tf.keras.layers.MaxPool1D(2)\n","        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='tanh')\n","        self.pool3 = tf.keras.layers.MaxPool1D(2)\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.fullyconn = tf.keras.layers.Dense(1024, activation='relu')\n","\n","        self.out = tf.keras.layers.Dense(units=10, activation=None)\n","\n","    def call(self, x):\n","        o_conv1 = self.conv1(x)\n","        o_pool1 = self.pool1(o_conv1)\n","        o_conv2 = self.conv2(o_pool1)\n","        o_pool2 = self.pool2(o_conv2)\n","        o_conv3 = self.conv3(o_pool2)\n","        o_pool3 = self.pool3(o_conv3)\n","        o_flat = self.flatten(o_pool3)\n","        o_dropout = self.dropout(o_flat)\n","        #o_fc = self.fullyconn(o_dropout)\n","        logits = self.out(o_dropout)\n","        y_pred = tf.nn.softmax(logits)\n","\n","        return y_pred, logits\n","\n","\n","# accuracy\n","def compute_accuracy(y_pred, y):\n","    corr_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n","    acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n","    return acc\n","\n","\n","# loss function\n","def cross_entropy_loss(logits, y):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","\n","\n","#optimisation\n","optimiser = tf.keras.optimizers.legacy.Adam(1e-4)\n","def tune_param(model, x, y):\n","    with tf.GradientTape() as tape:\n","        y_pred, logits = model(x)\n","        loss = cross_entropy_loss(logits, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","\n","\n","# apply one-hot encoding to a target value\n","train_y, test_y = tf.one_hot(train_y, depth=10), tf.one_hot(test_y, depth=10)\n","\n","\n","# Run Experiment of 4 different modes\n","modes =  ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using different mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train data reconstruction (shuffle -> batch form)\n","    train_data = tf.data.Dataset.from_tensor_slices((Xtrain, train_y))\n","    train_data = train_data.repeat().shuffle(100).batch(50)\n","    train_data_iter = iter(train_data)\n","\n","    CNN_model = CNN()\n","    # optimise 200 epoch , but try optimise 10000 epoch in HPC\n","    for i in range(201):\n","        # assign each 50 data\n","        batch_x, batch_y = next(train_data_iter)\n","\n","        if i % 100 == 0:\n","            train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n","            print(\"%d epoch : accuracy %f\" % (i, train_accuracy))\n","\n","        # parameter update\n","        tune_param(CNN_model, batch_x, batch_y)\n","\n","    test_acc = compute_accuracy(CNN_model(Xtest)[0], test_y)\n","    print(\"accuracy : %f\" %test_acc)\n","    results[mode] = [test_acc*100]\n","\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"085cb5f4","metadata":{"id":"085cb5f4"},"source":["# (case4) dataset : review_test.csv (sample from original yelp data) & 3 convolution layers with relu & fully cnnected layer with tanh"]},{"cell_type":"code","execution_count":null,"id":"2fe49468","metadata":{"id":"2fe49468","outputId":"eb5ded63-1a06-4a0d-eaeb-fb0a38a3dfb8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/521737693.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["mode:  binary\n","0 epoch : accuracy 0.100000\n","100 epoch : accuracy 0.540000\n","200 epoch : accuracy 0.740000\n","accuracy : 0.547739\n","mode:  count\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.580000\n","200 epoch : accuracy 0.880000\n","accuracy : 0.462312\n","mode:  tfidf\n","0 epoch : accuracy 0.060000\n","100 epoch : accuracy 0.880000\n","200 epoch : accuracy 0.960000\n","accuracy : 0.542714\n","mode:  freq\n","0 epoch : accuracy 0.060000\n","100 epoch : accuracy 0.460000\n","200 epoch : accuracy 0.580000\n","accuracy : 0.517588\n","\n","                                         binary  \\\n","0  tf.Tensor(54.77387, shape=(), dtype=float32)   \n","\n","                                           count  \\\n","0  tf.Tensor(46.231155, shape=(), dtype=float32)   \n","\n","                                          tfidf  \\\n","0  tf.Tensor(54.27136, shape=(), dtype=float32)   \n","\n","                                            freq  \n","0  tf.Tensor(51.758797, shape=(), dtype=float32)  \n"]}],"source":["#########################\n","# Define the vocabulary #\n","#########################\n","review = pd.read_csv('review_test.csv')\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","del review['Unnamed: 0']\n","\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import MinMaxScaler\n","\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","\n","def add_doc_to_vocab(docs, vocab):\n","\n","    # docs: a list of sentences (docs)\n","    # vocab: a vocabulary dictionary\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab # updated vocabulary\n","\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","\n","#########################\n","# embedding             #\n","#########################\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","\n","    if mode == 'count':\n","        scaler=MinMaxScaler()\n","        scaler.fit(Xtrain)\n","        Xtrain=scaler.transform(Xtrain)\n","        scaler.fit(Xtest)\n","        Xtest=scaler.transform(Xtest)\n","\n","    return Xtrain, Xtest\n","\n","\n","#########################\n","# CNN                   #\n","#########################\n","class CNN(tf.keras.Model):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='relu')\n","        self.pool1 = tf.keras.layers.MaxPool1D(2)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu')\n","        self.pool2 = tf.keras.layers.MaxPool1D(2)\n","        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu')\n","        self.pool3 = tf.keras.layers.MaxPool1D(2)\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.fullyconn = tf.keras.layers.Dense(1024, activation='tanh')\n","\n","        self.out = tf.keras.layers.Dense(units=10, activation=None)\n","\n","    def call(self, x):\n","        o_conv1 = self.conv1(x)\n","        o_pool1 = self.pool1(o_conv1)\n","        o_conv2 = self.conv2(o_pool1)\n","        o_pool2 = self.pool2(o_conv2)\n","        o_conv3 = self.conv3(o_pool2)\n","        o_pool3 = self.pool3(o_conv3)\n","        o_flat = self.flatten(o_pool3)\n","        o_dropout = self.dropout(o_flat)\n","        #o_fc = self.fullyconn(o_dropout)\n","        logits = self.out(o_dropout)\n","        y_pred = tf.nn.softmax(logits)\n","\n","        return y_pred, logits\n","\n","\n","# accuracy\n","def compute_accuracy(y_pred, y):\n","    corr_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n","    acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n","    return acc\n","\n","\n","# loss function\n","def cross_entropy_loss(logits, y):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","\n","\n","# optimisation\n","optimiser = tf.keras.optimizers.legacy.Adam(1e-4)\n","def tune_param(model, x, y):\n","    with tf.GradientTape() as tape:\n","        y_pred, logits = model(x)\n","        loss = cross_entropy_loss(logits, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","\n","\n","# apply one-hot encoding to a target value\n","train_y, test_y = tf.one_hot(train_y, depth=10), tf.one_hot(test_y, depth=10)\n","\n","\n","# Run Experiment of 4 different modes\n","modes =  ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using different mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train data reconstruction (shuffle -> batch form)\n","    train_data = tf.data.Dataset.from_tensor_slices((Xtrain, train_y))\n","    train_data = train_data.repeat().shuffle(100).batch(50)\n","    train_data_iter = iter(train_data)\n","\n","    CNN_model = CNN()\n","    # optimise 200 epoch , but try optimise 10000 epoch in HPC\n","    for i in range(201):\n","        # assign each 50 data\n","        batch_x, batch_y = next(train_data_iter)\n","\n","        if i % 100 == 0:\n","            train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n","            print(\"%d epoch : accuracy %f\" % (i, train_accuracy))\n","\n","        # parameter update\n","        tune_param(CNN_model, batch_x, batch_y)\n","\n","    test_acc = compute_accuracy(CNN_model(Xtest)[0], test_y)\n","    print(\"accuracy : %f\" %test_acc)\n","    results[mode] = [test_acc*100]\n","\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"a5ffdb3e","metadata":{"id":"a5ffdb3e"},"source":["# additional exp - dataset : review_test & using stride and padding"]},{"cell_type":"code","execution_count":null,"id":"6b903393","metadata":{"id":"6b903393","outputId":"9463407e-a03b-404b-cc0a-6e50763be074"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/3572518963.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["mode:  binary\n","0 epoch : accuracy 0.060000\n","100 epoch : accuracy 0.440000\n","200 epoch : accuracy 0.860000\n","accuracy : 0.557789\n","mode:  count\n","0 epoch : accuracy 0.080000\n","100 epoch : accuracy 0.680000\n","200 epoch : accuracy 0.900000\n","accuracy : 0.517588\n","mode:  tfidf\n","0 epoch : accuracy 0.040000\n","100 epoch : accuracy 0.940000\n","200 epoch : accuracy 0.980000\n","accuracy : 0.587940\n","mode:  freq\n","0 epoch : accuracy 0.020000\n","100 epoch : accuracy 0.480000\n","200 epoch : accuracy 0.580000\n","accuracy : 0.517588\n","\n","                                          binary  \\\n","0  tf.Tensor(55.778896, shape=(), dtype=float32)   \n","\n","                                           count  \\\n","0  tf.Tensor(51.758797, shape=(), dtype=float32)   \n","\n","                                          tfidf  \\\n","0  tf.Tensor(58.79397, shape=(), dtype=float32)   \n","\n","                                            freq  \n","0  tf.Tensor(51.758797, shape=(), dtype=float32)  \n"]}],"source":["#########################\n","# Define the vocabulary #\n","#########################\n","review = pd.read_csv('review_test.csv')\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","#del review['review_id']\n","#del review['user_id']\n","#del review['business_id']\n","#del review['useful']\n","#del review['funny']\n","#del review['cool']\n","#del review['date']\n","#del review['state']\n","#del review['Unnamed: 0.1']\n","del review['Unnamed: 0']\n","\n","\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","from sklearn.preprocessing import MinMaxScaler\n","\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","def add_doc_to_vocab(docs, vocab):\n","\n","    # docs: a list of sentences (docs)\n","    # vocab: a vocabulary dictionary\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab # updated vocabulary\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","\n","    if mode == 'count':\n","        scaler=MinMaxScaler()\n","        scaler.fit(Xtrain)\n","        Xtrain=scaler.transform(Xtrain)\n","        scaler.fit(Xtest)\n","        Xtest=scaler.transform(Xtest)\n","\n","    return Xtrain, Xtest\n","\n","\n","class CNN(tf.keras.Model):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","\n","        self.conv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=5, activation='tanh', strides=1, padding='same')\n","        self.pool1 = tf.keras.layers.MaxPool1D(2)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='tanh', strides=1, padding='same')\n","        self.pool2 = tf.keras.layers.MaxPool1D(2)\n","        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='tanh', strides=1, padding='same')\n","        self.pool3 = tf.keras.layers.MaxPool1D(2)\n","\n","        self.flatten = tf.keras.layers.Flatten()\n","        self.dropout = tf.keras.layers.Dropout(0.5)\n","        self.fullyconn = tf.keras.layers.Dense(1024, activation='relu')\n","\n","        self.out = tf.keras.layers.Dense(units=10, activation=None)\n","\n","    def call(self, x):\n","        #x = tf.reshape(x, [-1,28,28,1])\n","        o_conv1 = self.conv1(x)\n","        o_pool1 = self.pool1(o_conv1)\n","        o_conv2 = self.conv2(o_pool1)\n","        o_pool2 = self.pool2(o_conv2)\n","        o_conv3 = self.conv3(o_pool2)\n","        o_pool3 = self.pool3(o_conv3)\n","        o_flat = self.flatten(o_pool3)\n","        o_dropout = self.dropout(o_flat)\n","        #o_fc = self.fullyconn(o_dropout)\n","        logits = self.out(o_dropout)\n","        y_pred = tf.nn.softmax(logits)\n","\n","        return y_pred, logits\n","\n","\n","\n","def compute_accuracy(y_pred, y):\n","    corr_pred = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n","    acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n","\n","    return acc\n","\n","\n","def cross_entropy_loss(logits, y):\n","    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n","\n","\n","#optimiser = tf.keras.optimizers.Adam(1e-4)\n","optimiser = tf.keras.optimizers.legacy.Adam(1e-4)\n","def tune_param(model, x, y):\n","    with tf.GradientTape() as tape:\n","        y_pred, logits = model(x)\n","        loss = cross_entropy_loss(logits, y)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    #tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n","    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","\n","\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","\n","# apply one-hot encoding to a target value\n","train_y, test_y = tf.one_hot(train_y, depth=10), tf.one_hot(test_y, depth=10)\n","\n","\n","# Run Experiment of 4 different modes\n","# !!!!(Nina) 'count', 'tfidf', 'freq' -> need to normalise\n","modes =  ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using freq mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train data reconstruction (shuffle -> batch form)\n","    train_data = tf.data.Dataset.from_tensor_slices((Xtrain, train_y))\n","    train_data = train_data.repeat().shuffle(100).batch(50)\n","    train_data_iter = iter(train_data)\n","\n","    CNN_model = CNN()\n","    # optimise 10000 epoch\n","    for i in range(201):\n","        # assign each 50 data\n","        batch_x, batch_y = next(train_data_iter)\n","\n","\n","        if i % 100 == 0:\n","            train_accuracy = compute_accuracy(CNN_model(batch_x)[0], batch_y)\n","            print(\"%d epoch : accuracy %f\" % (i, train_accuracy))\n","\n","        # parameter update\n","        tune_param(CNN_model, batch_x, batch_y)\n","\n","    test_acc = compute_accuracy(CNN_model(Xtest)[0], test_y)\n","    print(\"accuracy : %f\" %test_acc)\n","    results[mode] = [test_acc*100]\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"386f2bc1","metadata":{"id":"386f2bc1"},"source":["# additional exp - dataset : review_test (same as 1st model) & only 1 convolution"]},{"cell_type":"code","execution_count":null,"id":"15c44de1","metadata":{"id":"15c44de1","outputId":"e61f45ce-3caa-41cd-ebbe-5f31b1a8dd29"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/3476246864.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["train_x size:  800\n","train_y size:  800\n","test_x size:  199\n","test_y size:  199\n","mode:  binary\n","Epoch 1/10\n","16/16 - 10s - loss: 1.3761 - accuracy: 0.5163 - 10s/epoch - 649ms/step\n","Epoch 2/10\n","16/16 - 7s - loss: 0.9059 - accuracy: 0.6500 - 7s/epoch - 460ms/step\n","Epoch 3/10\n","16/16 - 8s - loss: 0.5882 - accuracy: 0.8050 - 8s/epoch - 471ms/step\n","Epoch 4/10\n","16/16 - 7s - loss: 0.3765 - accuracy: 0.8938 - 7s/epoch - 447ms/step\n","Epoch 5/10\n","16/16 - 7s - loss: 0.2314 - accuracy: 0.9513 - 7s/epoch - 463ms/step\n","Epoch 6/10\n","16/16 - 8s - loss: 0.1509 - accuracy: 0.9775 - 8s/epoch - 473ms/step\n","Epoch 7/10\n","16/16 - 7s - loss: 0.0987 - accuracy: 0.9875 - 7s/epoch - 444ms/step\n","Epoch 8/10\n","16/16 - 7s - loss: 0.0702 - accuracy: 0.9925 - 7s/epoch - 448ms/step\n","Epoch 9/10\n","16/16 - 8s - loss: 0.0499 - accuracy: 0.9975 - 8s/epoch - 469ms/step\n","Epoch 10/10\n","16/16 - 7s - loss: 0.0356 - accuracy: 0.9987 - 7s/epoch - 443ms/step\n","Test Accuracy: 65.32663106918335\n","mode:  count\n","Epoch 1/10\n","16/16 - 9s - loss: 1.3892 - accuracy: 0.5400 - 9s/epoch - 554ms/step\n","Epoch 2/10\n","16/16 - 936s - loss: 0.9173 - accuracy: 0.6513 - 936s/epoch - 59s/step\n","Epoch 3/10\n","16/16 - 8s - loss: 0.6285 - accuracy: 0.7725 - 8s/epoch - 515ms/step\n","Epoch 4/10\n","16/16 - 10s - loss: 0.3989 - accuracy: 0.8875 - 10s/epoch - 616ms/step\n","Epoch 5/10\n","16/16 - 7s - loss: 0.2625 - accuracy: 0.9400 - 7s/epoch - 449ms/step\n","Epoch 6/10\n","16/16 - 7s - loss: 0.1674 - accuracy: 0.9737 - 7s/epoch - 413ms/step\n","Epoch 7/10\n","16/16 - 6s - loss: 0.1119 - accuracy: 0.9887 - 6s/epoch - 406ms/step\n","Epoch 8/10\n","16/16 - 3349s - loss: 0.0778 - accuracy: 0.9925 - 3349s/epoch - 209s/step\n","Epoch 9/10\n","16/16 - 8s - loss: 0.0574 - accuracy: 0.9962 - 8s/epoch - 512ms/step\n","Epoch 10/10\n","16/16 - 11s - loss: 0.0426 - accuracy: 0.9975 - 11s/epoch - 671ms/step\n","Test Accuracy: 63.81909251213074\n","mode:  tfidf\n","Epoch 1/10\n","16/16 - 8s - loss: 1.3942 - accuracy: 0.5362 - 8s/epoch - 479ms/step\n","Epoch 2/10\n","16/16 - 7s - loss: 0.4896 - accuracy: 0.8338 - 7s/epoch - 410ms/step\n","Epoch 3/10\n","16/16 - 4665s - loss: 0.1997 - accuracy: 0.9513 - 4665s/epoch - 292s/step\n","Epoch 4/10\n","16/16 - 9s - loss: 0.0935 - accuracy: 0.9850 - 9s/epoch - 577ms/step\n","Epoch 5/10\n","16/16 - 12s - loss: 0.0467 - accuracy: 0.9950 - 12s/epoch - 720ms/step\n","Epoch 6/10\n","16/16 - 9s - loss: 0.0276 - accuracy: 0.9987 - 9s/epoch - 567ms/step\n","Epoch 7/10\n","16/16 - 7s - loss: 0.0187 - accuracy: 1.0000 - 7s/epoch - 457ms/step\n","Epoch 8/10\n","16/16 - 7s - loss: 0.0141 - accuracy: 1.0000 - 7s/epoch - 437ms/step\n","Epoch 9/10\n","16/16 - 1495s - loss: 0.0090 - accuracy: 1.0000 - 1495s/epoch - 93s/step\n","Epoch 10/10\n","16/16 - 15s - loss: 0.0070 - accuracy: 1.0000 - 15s/epoch - 909ms/step\n","Test Accuracy: 64.3216073513031\n","mode:  freq\n","Epoch 1/10\n","16/16 - 15s - loss: 1.4367 - accuracy: 0.5100 - 15s/epoch - 936ms/step\n","Epoch 2/10\n","16/16 - 9s - loss: 1.3055 - accuracy: 0.5263 - 9s/epoch - 590ms/step\n","Epoch 3/10\n","16/16 - 9s - loss: 1.2994 - accuracy: 0.5263 - 9s/epoch - 545ms/step\n","Epoch 4/10\n","16/16 - 8s - loss: 1.2688 - accuracy: 0.5263 - 8s/epoch - 509ms/step\n","Epoch 5/10\n","16/16 - 7s - loss: 1.2533 - accuracy: 0.5263 - 7s/epoch - 461ms/step\n","Epoch 6/10\n","16/16 - 7s - loss: 1.2485 - accuracy: 0.5263 - 7s/epoch - 465ms/step\n","Epoch 7/10\n","16/16 - 7s - loss: 1.2075 - accuracy: 0.5263 - 7s/epoch - 448ms/step\n","Epoch 8/10\n","16/16 - 12s - loss: 1.1679 - accuracy: 0.5288 - 12s/epoch - 764ms/step\n","Epoch 9/10\n","16/16 - 10s - loss: 1.1250 - accuracy: 0.5400 - 10s/epoch - 613ms/step\n","Epoch 10/10\n","16/16 - 9s - loss: 1.0893 - accuracy: 0.5800 - 9s/epoch - 558ms/step\n","Test Accuracy: 52.76381969451904\n","\n","      binary      count      tfidf      freq\n","0  65.326631  63.819093  64.321607  52.76382\n"]}],"source":["#########################\n","# Define the vocabulary #\n","#########################\n","review = pd.read_csv('review_test.csv')\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","#del review['review_id']\n","#del review['user_id']\n","#del review['business_id']\n","#del review['useful']\n","#del review['funny']\n","#del review['cool']\n","#del review['date']\n","#del review['state']\n","#del review['Unnamed: 0.1']\n","del review['Unnamed: 0']\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","def add_doc_to_vocab(docs, vocab):\n","    '''\n","    input:\n","        docs: a list of sentences (docs)\n","        vocab: a vocabulary dictionary\n","    output:\n","        return an updated vocabulary\n","    '''\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","    return Xtrain, Xtest\n","\n","def train_cnn(train_x, train_y, batch_size = 50, epochs = 10, verbose =2):\n","\n","    n_words = train_x.shape[1]\n","\n","    # n_words : the number of vocab\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Conv1D(filters=100, kernel_size=5, activation='relu', input_shape=(n_words,1)),\n","        tf.keras.layers.MaxPool1D(2),\n","\n","        # !!!! (Nina) more kernel layers!!!\n","        #tf.keras.layers.Conv1D(filters=100, kernel_size=5, activation='relu', input_shape=(n_words,1)),\n","        # tf.keras.layers.MaxPool1D(2),\n","\n","        tf.keras.layers.Flatten(),\n","\n","        tf.keras.layers.Dropout(0.5),\n","\n","        # !!!! (Nina) 3 convolution layers & 3 dense layers\n","        tf.keras.layers.Dense( units=6, activation='softmax')\n","    ])\n","\n","    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","    model.fit(train_x, train_y, batch_size, epochs, verbose)\n","    return model\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","print('train_x size: ', len(train_x))\n","print('train_y size: ', len(train_y))\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","print('test_x size: ', len(test_x))\n","print('test_y size: ', len(test_y))\n","\n","# Run Experiment of 4 different modes\n","# !!!!(Nina) 'count', 'tfidf', 'freq' -> need to normalise\n","modes = ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using freq mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train the model\n","    model = train_cnn(Xtrain, train_y)\n","\n","    # evaluate the model\n","    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n","    print('Test Accuracy: {}'.format(acc*100))\n","    results[mode] = [acc*100]\n","\n","print()\n","print(results)"]},{"cell_type":"markdown","id":"c06a0cf2","metadata":{"id":"c06a0cf2"},"source":["# additional exp - dataset : output_reviews_PA_FL (same as 1st model) & only 1 convolution"]},{"cell_type":"code","execution_count":null,"id":"01f79b7f","metadata":{"id":"01f79b7f","outputId":"61a00bfb-c9e5-4c9e-a8e4-b2894247ce0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/gs/qkvt6xzd2p17cfbwj2f8vs5h0000gn/T/ipykernel_42346/3400335869.py:19: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  review['split'][800:1000] = 'test'\n"]},{"name":"stdout","output_type":"stream","text":["train_x size:  800\n","train_y size:  800\n","test_x size:  200\n","test_y size:  200\n","mode:  binary\n","Epoch 1/10\n","16/16 - 8s - loss: 1.3919 - accuracy: 0.4850 - 8s/epoch - 528ms/step\n","Epoch 2/10\n","16/16 - 8s - loss: 0.9395 - accuracy: 0.6087 - 8s/epoch - 471ms/step\n","Epoch 3/10\n","16/16 - 7s - loss: 0.6210 - accuracy: 0.8050 - 7s/epoch - 422ms/step\n","Epoch 4/10\n","16/16 - 8s - loss: 0.4098 - accuracy: 0.8950 - 8s/epoch - 495ms/step\n","Epoch 5/10\n","16/16 - 11s - loss: 0.2621 - accuracy: 0.9538 - 11s/epoch - 673ms/step\n","Epoch 6/10\n","16/16 - 9s - loss: 0.1737 - accuracy: 0.9750 - 9s/epoch - 585ms/step\n","Epoch 7/10\n","16/16 - 9s - loss: 0.1238 - accuracy: 0.9887 - 9s/epoch - 586ms/step\n","Epoch 8/10\n","16/16 - 8s - loss: 0.0886 - accuracy: 0.9925 - 8s/epoch - 496ms/step\n","Epoch 9/10\n","16/16 - 7s - loss: 0.0669 - accuracy: 0.9962 - 7s/epoch - 458ms/step\n","Epoch 10/10\n","16/16 - 8s - loss: 0.0509 - accuracy: 0.9975 - 8s/epoch - 480ms/step\n","Test Accuracy: 43.50000023841858\n","mode:  count\n","Epoch 1/10\n","16/16 - 19s - loss: 1.4122 - accuracy: 0.4762 - 19s/epoch - 1s/step\n","Epoch 2/10\n","16/16 - 10s - loss: 0.9133 - accuracy: 0.6237 - 10s/epoch - 654ms/step\n","Epoch 3/10\n","16/16 - 8s - loss: 0.6387 - accuracy: 0.7775 - 8s/epoch - 512ms/step\n","Epoch 4/10\n","16/16 - 9s - loss: 0.4289 - accuracy: 0.8788 - 9s/epoch - 549ms/step\n","Epoch 5/10\n","16/16 - 9s - loss: 0.2957 - accuracy: 0.9312 - 9s/epoch - 565ms/step\n","Epoch 6/10\n","16/16 - 9s - loss: 0.2029 - accuracy: 0.9613 - 9s/epoch - 557ms/step\n","Epoch 7/10\n","16/16 - 8s - loss: 0.1421 - accuracy: 0.9837 - 8s/epoch - 500ms/step\n","Epoch 8/10\n","16/16 - 7s - loss: 0.1013 - accuracy: 0.9925 - 7s/epoch - 410ms/step\n","Epoch 9/10\n","16/16 - 7s - loss: 0.0779 - accuracy: 0.9962 - 7s/epoch - 414ms/step\n","Epoch 10/10\n","16/16 - 7s - loss: 0.0596 - accuracy: 0.9987 - 7s/epoch - 444ms/step\n","Test Accuracy: 40.00000059604645\n","mode:  tfidf\n","Epoch 1/10\n","16/16 - 9s - loss: 1.4501 - accuracy: 0.4625 - 9s/epoch - 533ms/step\n","Epoch 2/10\n","16/16 - 7s - loss: 0.5231 - accuracy: 0.8425 - 7s/epoch - 449ms/step\n","Epoch 3/10\n","16/16 - 7s - loss: 0.2340 - accuracy: 0.9425 - 7s/epoch - 465ms/step\n","Epoch 4/10\n","16/16 - 8s - loss: 0.1168 - accuracy: 0.9837 - 8s/epoch - 491ms/step\n","Epoch 5/10\n","16/16 - 7s - loss: 0.0658 - accuracy: 0.9937 - 7s/epoch - 456ms/step\n","Epoch 6/10\n","16/16 - 7s - loss: 0.0424 - accuracy: 0.9962 - 7s/epoch - 445ms/step\n","Epoch 7/10\n","16/16 - 7s - loss: 0.0282 - accuracy: 0.9987 - 7s/epoch - 446ms/step\n","Epoch 8/10\n","16/16 - 6s - loss: 0.0195 - accuracy: 0.9987 - 6s/epoch - 390ms/step\n","Epoch 9/10\n","16/16 - 6s - loss: 0.0144 - accuracy: 1.0000 - 6s/epoch - 384ms/step\n","Epoch 10/10\n","16/16 - 7s - loss: 0.0111 - accuracy: 1.0000 - 7s/epoch - 422ms/step\n","Test Accuracy: 38.999998569488525\n","mode:  freq\n","Epoch 1/10\n","16/16 - 8s - loss: 1.4117 - accuracy: 0.4837 - 8s/epoch - 469ms/step\n","Epoch 2/10\n","16/16 - 7s - loss: 1.3009 - accuracy: 0.5050 - 7s/epoch - 416ms/step\n","Epoch 3/10\n","16/16 - 7s - loss: 1.3157 - accuracy: 0.5050 - 7s/epoch - 411ms/step\n","Epoch 4/10\n","16/16 - 7s - loss: 1.2826 - accuracy: 0.5050 - 7s/epoch - 411ms/step\n","Epoch 5/10\n","16/16 - 7s - loss: 1.2703 - accuracy: 0.5050 - 7s/epoch - 411ms/step\n","Epoch 6/10\n","16/16 - 7s - loss: 1.2649 - accuracy: 0.5050 - 7s/epoch - 425ms/step\n","Epoch 7/10\n","16/16 - 7s - loss: 1.2386 - accuracy: 0.5050 - 7s/epoch - 433ms/step\n","Epoch 8/10\n","16/16 - 6s - loss: 1.2076 - accuracy: 0.5075 - 6s/epoch - 404ms/step\n","Epoch 9/10\n","16/16 - 7s - loss: 1.1699 - accuracy: 0.5075 - 7s/epoch - 418ms/step\n","Epoch 10/10\n","16/16 - 6s - loss: 1.1344 - accuracy: 0.5163 - 6s/epoch - 389ms/step\n","Test Accuracy: 24.500000476837158\n","\n","   binary      count      tfidf  freq\n","0    43.5  40.000001  38.999999  24.5\n"]}],"source":["#########################\n","# Define the vocabulary #\n","#########################\n","review = pd.read_csv('output_reviews_PA_FL.csv')\n","\n","del review['review_id']\n","del review['user_id']\n","del review['business_id']\n","del review['useful']\n","del review['funny']\n","del review['cool']\n","del review['date']\n","del review['state']\n","del review['Unnamed: 0.1']\n","del review['Unnamed: 0']\n","\n","review = review[:1000]\n","review['split'] = 'train'\n","review['split'][800:1000] = 'test'\n","\n","from collections import Counter\n","from nltk.corpus import stopwords\n","stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","def clean_doc(doc):\n","    # split into tokens by white space\n","    tokens = doc.split()\n","    # prepare regex for char filtering\n","    re_punc = re.compile('[%s]' % re.escape(punctuation))\n","    # remove punctuation from each word\n","    tokens = [re_punc.sub('', w) for w in tokens]\n","    # filter out stop words\n","    tokens = [w for w in tokens if not w in stopwords]\n","    # filter out short tokens\n","    tokens = [word for word in tokens if len(word) >= 1]\n","    # Stem the token\n","    tokens = [stemmer.stem(token) for token in tokens]\n","    return tokens\n","\n","def add_doc_to_vocab(docs, vocab):\n","    '''\n","    input:\n","        docs: a list of sentences (docs)\n","        vocab: a vocabulary dictionary\n","    output:\n","        return an updated vocabulary\n","    '''\n","    for doc in docs:\n","        tokens = clean_doc(doc)\n","        vocab.update(tokens)\n","    return vocab\n","\n","def doc_to_line(doc, vocab):\n","    tokens = clean_doc(doc)\n","    # filter by vocab\n","    tokens = [token for token in tokens if token in vocab]\n","    line = ' '.join(tokens)\n","    return line\n","\n","def clean_docs(docs, vocab):\n","    lines = []\n","    for doc in docs:\n","        line = doc_to_line(doc, vocab)\n","        lines.append(line)\n","    return lines\n","\n","# prepare bag-of-words encoding of docs\n","def prepare_data(train_docs, test_docs, mode):\n","    # create the tokenizer\n","    tokenizer = Tokenizer()\n","    # fit the tokenizer on the documents\n","    tokenizer.fit_on_texts(train_docs)\n","    #tokenizer.fit_on_texts(test_docs)\n","    #print(train_docs[0])\n","    # encode training data set\n","    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n","    #print(Xtrain[0])\n","    # encode test data set\n","    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n","    return Xtrain, Xtest\n","\n","def train_cnn(train_x, train_y, batch_size = 50, epochs = 10, verbose =2):\n","\n","    n_words = train_x.shape[1]\n","\n","    # n_words : the number of vocab\n","    model = tf.keras.models.Sequential([\n","        tf.keras.layers.Conv1D(filters=100, kernel_size=5, activation='relu', input_shape=(n_words,1)),\n","        tf.keras.layers.MaxPool1D(2),\n","\n","        # !!!! (Nina) more kernel layers!!!\n","        #tf.keras.layers.Conv1D(filters=100, kernel_size=5, activation='relu', input_shape=(n_words,1)),\n","        # tf.keras.layers.MaxPool1D(2),\n","\n","        tf.keras.layers.Flatten(),\n","\n","        tf.keras.layers.Dropout(0.5),\n","\n","        # !!!! (Nina) 3 convolution layers & 3 dense layers\n","        tf.keras.layers.Dense( units=6, activation='softmax')\n","    ])\n","\n","    model.compile( loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","    model.fit(train_x, train_y, batch_size, epochs, verbose)\n","    return model\n","\n","# Separate the sentences and the labels for training and testing\n","train_x = list(review[review.split=='train'].text)\n","train_y = np.array(review[review.split=='train'].stars)\n","print('train_x size: ', len(train_x))\n","print('train_y size: ', len(train_y))\n","\n","test_x = list(review[review.split=='test'].text)\n","test_y = np.array(review[review.split=='test'].stars)\n","print('test_x size: ', len(test_x))\n","print('test_y size: ', len(test_y))\n","\n","# Run Experiment of 4 different modes\n","# !!!!(Nina) 'count', 'tfidf', 'freq' -> need to normalise\n","modes = ['binary', 'count', 'tfidf', 'freq']\n","results = pd.DataFrame()\n","\n","for mode in modes:\n","    print('mode: ', mode)\n","\n","    # Instantiate a vocab object\n","    vocab = Counter()\n","\n","    # Define a vocabulary for each fold\n","    vocab = add_doc_to_vocab(train_x, vocab)\n","\n","    # Clean the sentences\n","    train_x = clean_docs(train_x, vocab)\n","    test_x = clean_docs(test_x, vocab)\n","\n","    # encode data using freq mode\n","    Xtrain, Xtest = prepare_data(train_x, test_x, mode)\n","\n","    Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], 1))\n","    Xtest = np.reshape(Xtest, (Xtest.shape[0], Xtest.shape[1], 1))\n","\n","    # train the model\n","    model = train_cnn(Xtrain, train_y)\n","\n","    # evaluate the model\n","    loss, acc = model.evaluate(Xtest, test_y, verbose=0)\n","    print('Test Accuracy: {}'.format(acc*100))\n","    results[mode] = [acc*100]\n","\n","print()\n","print(results)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}